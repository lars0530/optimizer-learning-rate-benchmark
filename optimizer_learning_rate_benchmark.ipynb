{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nUaXc_-hGMmN"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hckfUHjWnjCN"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "numpy.ufunc size changed, may indicate binary incompatibility. Expected 232 from C header, got 216 from PyObject",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/rv/glgm061s4pb7vq0r2d3_hp0c0000gn/T/ipykernel_96881/1842950122.py\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent_processing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mevent_accumulator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/tf2_lars/lib/python3.10/site-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0m_distributor_init\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     )\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/tf2_lars/lib/python3.10/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInconsistentVersionWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_IS_32BIT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_estimator_html_repr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_HTMLDocumentationLinkMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_html_repr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata_requests\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_MetadataRequester\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_routing_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/tf2_lars/lib/python3.10/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdiscovery\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mall_estimators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparse_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreadpool_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmurmurhash\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmurmurhash3_32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m from .validation import (\n\u001b[1;32m     30\u001b[0m     \u001b[0m_is_arraylike_not_scalar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32msklearn/utils/murmurhash.pyx\u001b[0m in \u001b[0;36minit sklearn.utils.murmurhash\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.ufunc size changed, may indicate binary incompatibility. Expected 232 from C header, got 216 from PyObject"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import os\n",
        "import datetime\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import pandas as pd\n",
        "from tensorboard.backend.event_processing import event_accumulator\n",
        "\n",
        "# =======================\n",
        "# Initialization Section\n",
        "# =======================\n",
        "\n",
        "# Load the MNIST dataset\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor())\n",
        "\n",
        "train_batch_size = 64  # Size of batches for training\n",
        "\n",
        "# Split the training dataset into training and validation sets\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_subset, val_subset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_subset, batch_size=train_batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_subset, batch_size=1000, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "# Define a simple neural network\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 128)  # First fully connected layer (input: 28x28 pixels, output: 128 units)\n",
        "        self.fc2 = nn.Linear(128, 10)     # Second fully connected layer (input: 128 units, output: 10 units for 10 classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)             # Flatten the input image\n",
        "        x = torch.relu(self.fc1(x))       # Apply ReLU activation to the output of the first layer\n",
        "        x = self.fc2(x)                   # Output layer\n",
        "        return x\n",
        "\n",
        "# List of optimizers with names\n",
        "optimizers = [\n",
        "    (\"SGD\", lambda params: optim.SGD(params, lr=0.1)),  # Standard SGD\n",
        "    (\"SGD-Momentum\", lambda params: optim.SGD(params, lr=0.01, momentum=0.9)),  # Momentum\n",
        "    (\"SGD-Nesterov\", lambda params: optim.SGD(params, lr=0.01, momentum=0.9, nesterov=True)),  # Nesterov\n",
        "    (\"Adagrad\", lambda params: optim.Adagrad(params, lr=0.01, )),  # Adagrad\n",
        "    (\"RMSprop\", lambda params: optim.RMSprop(params, lr=0.001)),  # RMSprop\n",
        "    (\"Adam\", lambda params: optim.Adam(params, lr=0.001)),  # Adam\n",
        "]\n",
        "\n",
        "# List of schedulers\n",
        "schedulers = [\n",
        "    lambda opt: lr_scheduler.ExponentialLR(opt, gamma=0.9),\n",
        "    lambda opt: lr_scheduler.OneCycleLR(opt, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=20, anneal_strategy='linear'),\n",
        "    lambda opt: lr_scheduler.StepLR(opt, step_size=5),\n",
        "    lambda opt: lr_scheduler.ReduceLROnPlateau(opt),\n",
        "    lambda opt: lr_scheduler.PolynomialLR(opt),\n",
        "    None  # No scheduler\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-P5uI57niqf",
        "outputId": "6feaa102-e184-4e91-b0b7-e97e5e374d6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Current Learning Rate: 0.004\n",
            "Epoch [1/20], Loss: 0.5883\n",
            "Epoch [1/20], Validation Accuracy: 90.95%, Validation Loss: 0.3074\n",
            "Epoch 2, Current Learning Rate: 0.004021338075127806\n",
            "Epoch [2/20], Loss: 0.2737\n",
            "Epoch [2/20], Validation Accuracy: 93.15%, Validation Loss: 0.2380\n",
            "Epoch 3, Current Learning Rate: 0.004042676150255612\n",
            "Epoch [3/20], Loss: 0.2174\n",
            "Epoch [3/20], Validation Accuracy: 94.46%, Validation Loss: 0.1955\n",
            "Epoch 4, Current Learning Rate: 0.004064014225383419\n",
            "Epoch [4/20], Loss: 0.1794\n",
            "Epoch [4/20], Validation Accuracy: 95.03%, Validation Loss: 0.1725\n",
            "Epoch 5, Current Learning Rate: 0.004085352300511225\n",
            "Epoch [5/20], Loss: 0.1516\n",
            "Epoch [5/20], Validation Accuracy: 95.40%, Validation Loss: 0.1594\n",
            "Epoch 6, Current Learning Rate: 0.004106690375639031\n",
            "Epoch [6/20], Loss: 0.1321\n",
            "Epoch [6/20], Validation Accuracy: 95.86%, Validation Loss: 0.1415\n",
            "Epoch 7, Current Learning Rate: 0.004128028450766837\n",
            "Epoch [7/20], Loss: 0.1160\n",
            "Epoch [7/20], Validation Accuracy: 96.37%, Validation Loss: 0.1287\n",
            "Epoch 8, Current Learning Rate: 0.004149366525894644\n",
            "Epoch [8/20], Loss: 0.1035\n",
            "Epoch [8/20], Validation Accuracy: 96.61%, Validation Loss: 0.1187\n",
            "Epoch 9, Current Learning Rate: 0.00417070460102245\n",
            "Epoch [9/20], Loss: 0.0929\n",
            "Epoch [9/20], Validation Accuracy: 96.78%, Validation Loss: 0.1138\n",
            "Epoch 10, Current Learning Rate: 0.004192042676150256\n",
            "Epoch [10/20], Loss: 0.0844\n",
            "Epoch [10/20], Validation Accuracy: 96.91%, Validation Loss: 0.1073\n",
            "Epoch 11, Current Learning Rate: 0.0042133807512780615\n",
            "Epoch [11/20], Loss: 0.0768\n",
            "Epoch [11/20], Validation Accuracy: 96.88%, Validation Loss: 0.1049\n",
            "Epoch 12, Current Learning Rate: 0.004234718826405868\n",
            "Epoch [12/20], Loss: 0.0703\n",
            "Epoch [12/20], Validation Accuracy: 97.05%, Validation Loss: 0.0999\n",
            "Epoch 13, Current Learning Rate: 0.004256056901533674\n",
            "Epoch [13/20], Loss: 0.0640\n",
            "Epoch [13/20], Validation Accuracy: 96.94%, Validation Loss: 0.1019\n",
            "Epoch 14, Current Learning Rate: 0.00427739497666148\n",
            "Epoch [14/20], Loss: 0.0592\n",
            "Epoch [14/20], Validation Accuracy: 97.15%, Validation Loss: 0.0974\n",
            "Epoch 15, Current Learning Rate: 0.004298733051789286\n",
            "Epoch [15/20], Loss: 0.0545\n",
            "Epoch [15/20], Validation Accuracy: 97.17%, Validation Loss: 0.0931\n",
            "Epoch 16, Current Learning Rate: 0.004320071126917093\n",
            "Epoch [16/20], Loss: 0.0504\n",
            "Epoch [16/20], Validation Accuracy: 97.33%, Validation Loss: 0.0919\n",
            "Epoch 17, Current Learning Rate: 0.004341409202044899\n",
            "Epoch [17/20], Loss: 0.0466\n",
            "Epoch [17/20], Validation Accuracy: 97.34%, Validation Loss: 0.0897\n",
            "Epoch 18, Current Learning Rate: 0.004362747277172705\n",
            "Epoch [18/20], Loss: 0.0436\n",
            "Epoch [18/20], Validation Accuracy: 97.40%, Validation Loss: 0.0882\n",
            "Epoch 19, Current Learning Rate: 0.004384085352300511\n",
            "Epoch [19/20], Loss: 0.0409\n",
            "Epoch [19/20], Validation Accuracy: 97.53%, Validation Loss: 0.0860\n",
            "Epoch 20, Current Learning Rate: 0.004405423427428318\n",
            "Epoch [20/20], Loss: 0.0381\n",
            "Epoch [20/20], Validation Accuracy: 97.45%, Validation Loss: 0.0866\n",
            "Accuracy on test set: 97.74%\n",
            "Precision on test set: 0.9774\n",
            "Recall on test set: 0.9771\n",
            "F1 Score on test set: 0.9772\n",
            "Data has been written to tensorboard_logs_SGD_OneCycleLR.xlsx\n",
            "Epoch 1, Current Learning Rate: 0.004\n",
            "Epoch [1/20], Loss: 0.5893\n",
            "Epoch [1/20], Validation Accuracy: 91.28%, Validation Loss: 0.2994\n",
            "Epoch 2, Current Learning Rate: 0.004021338075127806\n",
            "Epoch [2/20], Loss: 0.2766\n",
            "Epoch [2/20], Validation Accuracy: 93.20%, Validation Loss: 0.2377\n",
            "Epoch 3, Current Learning Rate: 0.004042676150255612\n",
            "Epoch [3/20], Loss: 0.2206\n",
            "Epoch [3/20], Validation Accuracy: 94.06%, Validation Loss: 0.2016\n",
            "Epoch 4, Current Learning Rate: 0.004064014225383419\n",
            "Epoch [4/20], Loss: 0.1814\n",
            "Epoch [4/20], Validation Accuracy: 94.90%, Validation Loss: 0.1758\n",
            "Epoch 5, Current Learning Rate: 0.004085352300511225\n",
            "Epoch [5/20], Loss: 0.1543\n",
            "Epoch [5/20], Validation Accuracy: 95.58%, Validation Loss: 0.1529\n",
            "Epoch 6, Current Learning Rate: 0.004106690375639031\n",
            "Epoch [6/20], Loss: 0.1327\n",
            "Epoch [6/20], Validation Accuracy: 95.90%, Validation Loss: 0.1392\n",
            "Epoch 7, Current Learning Rate: 0.004128028450766837\n",
            "Epoch [7/20], Loss: 0.1172\n",
            "Epoch [7/20], Validation Accuracy: 96.11%, Validation Loss: 0.1298\n",
            "Epoch 8, Current Learning Rate: 0.004149366525894644\n",
            "Epoch [8/20], Loss: 0.1034\n",
            "Epoch [8/20], Validation Accuracy: 96.33%, Validation Loss: 0.1207\n",
            "Epoch 9, Current Learning Rate: 0.00417070460102245\n",
            "Epoch [9/20], Loss: 0.0930\n",
            "Epoch [9/20], Validation Accuracy: 96.72%, Validation Loss: 0.1119\n",
            "Epoch 10, Current Learning Rate: 0.004192042676150256\n",
            "Epoch [10/20], Loss: 0.0840\n",
            "Epoch [10/20], Validation Accuracy: 96.81%, Validation Loss: 0.1079\n",
            "Epoch 11, Current Learning Rate: 0.0042133807512780615\n",
            "Epoch [11/20], Loss: 0.0771\n",
            "Epoch [11/20], Validation Accuracy: 96.83%, Validation Loss: 0.1050\n",
            "Epoch 12, Current Learning Rate: 0.004234718826405868\n",
            "Epoch [12/20], Loss: 0.0697\n",
            "Epoch [12/20], Validation Accuracy: 96.97%, Validation Loss: 0.1039\n",
            "Epoch 13, Current Learning Rate: 0.004256056901533674\n",
            "Epoch [13/20], Loss: 0.0638\n",
            "Epoch [13/20], Validation Accuracy: 97.26%, Validation Loss: 0.0974\n",
            "Epoch 14, Current Learning Rate: 0.00427739497666148\n",
            "Epoch [14/20], Loss: 0.0590\n",
            "Epoch [14/20], Validation Accuracy: 97.18%, Validation Loss: 0.0955\n",
            "Epoch 15, Current Learning Rate: 0.004298733051789286\n",
            "Epoch [15/20], Loss: 0.0556\n",
            "Epoch [15/20], Validation Accuracy: 97.29%, Validation Loss: 0.0932\n",
            "Epoch 16, Current Learning Rate: 0.004320071126917093\n",
            "Epoch [16/20], Loss: 0.0511\n",
            "Epoch [16/20], Validation Accuracy: 97.26%, Validation Loss: 0.0933\n",
            "Epoch 17, Current Learning Rate: 0.004341409202044899\n",
            "Epoch [17/20], Loss: 0.0476\n",
            "Epoch [17/20], Validation Accuracy: 97.26%, Validation Loss: 0.0916\n",
            "Epoch 18, Current Learning Rate: 0.004362747277172705\n",
            "Epoch [18/20], Loss: 0.0447\n",
            "Epoch [18/20], Validation Accuracy: 97.32%, Validation Loss: 0.0901\n",
            "Epoch 19, Current Learning Rate: 0.004384085352300511\n",
            "Epoch [19/20], Loss: 0.0418\n",
            "Epoch [19/20], Validation Accuracy: 97.33%, Validation Loss: 0.0903\n",
            "Epoch 20, Current Learning Rate: 0.004405423427428318\n",
            "Epoch [20/20], Loss: 0.0389\n",
            "Epoch [20/20], Validation Accuracy: 97.47%, Validation Loss: 0.0874\n",
            "Accuracy on test set: 97.77%\n",
            "Precision on test set: 0.9778\n",
            "Recall on test set: 0.9775\n",
            "F1 Score on test set: 0.9776\n",
            "Data has been written to tensorboard_logs_SGD-Momentum_OneCycleLR.xlsx\n",
            "Epoch 1, Current Learning Rate: 0.004\n",
            "Epoch [1/20], Loss: 0.5784\n",
            "Epoch [1/20], Validation Accuracy: 90.69%, Validation Loss: 0.3140\n",
            "Epoch 2, Current Learning Rate: 0.004021338075127806\n",
            "Epoch [2/20], Loss: 0.2791\n",
            "Epoch [2/20], Validation Accuracy: 92.94%, Validation Loss: 0.2465\n",
            "Epoch 3, Current Learning Rate: 0.004042676150255612\n",
            "Epoch [3/20], Loss: 0.2262\n",
            "Epoch [3/20], Validation Accuracy: 93.88%, Validation Loss: 0.2085\n",
            "Epoch 4, Current Learning Rate: 0.004064014225383419\n",
            "Epoch [4/20], Loss: 0.1881\n",
            "Epoch [4/20], Validation Accuracy: 94.83%, Validation Loss: 0.1792\n",
            "Epoch 5, Current Learning Rate: 0.004085352300511225\n",
            "Epoch [5/20], Loss: 0.1588\n",
            "Epoch [5/20], Validation Accuracy: 95.29%, Validation Loss: 0.1581\n",
            "Epoch 6, Current Learning Rate: 0.004106690375639031\n",
            "Epoch [6/20], Loss: 0.1368\n",
            "Epoch [6/20], Validation Accuracy: 95.87%, Validation Loss: 0.1406\n",
            "Epoch 7, Current Learning Rate: 0.004128028450766837\n",
            "Epoch [7/20], Loss: 0.1200\n",
            "Epoch [7/20], Validation Accuracy: 96.20%, Validation Loss: 0.1314\n",
            "Epoch 8, Current Learning Rate: 0.004149366525894644\n",
            "Epoch [8/20], Loss: 0.1064\n",
            "Epoch [8/20], Validation Accuracy: 96.41%, Validation Loss: 0.1221\n",
            "Epoch 9, Current Learning Rate: 0.00417070460102245\n",
            "Epoch [9/20], Loss: 0.0955\n",
            "Epoch [9/20], Validation Accuracy: 96.53%, Validation Loss: 0.1153\n",
            "Epoch 10, Current Learning Rate: 0.004192042676150256\n",
            "Epoch [10/20], Loss: 0.0867\n",
            "Epoch [10/20], Validation Accuracy: 96.80%, Validation Loss: 0.1104\n",
            "Epoch 11, Current Learning Rate: 0.0042133807512780615\n",
            "Epoch [11/20], Loss: 0.0783\n",
            "Epoch [11/20], Validation Accuracy: 96.91%, Validation Loss: 0.1061\n",
            "Epoch 12, Current Learning Rate: 0.004234718826405868\n",
            "Epoch [12/20], Loss: 0.0723\n",
            "Epoch [12/20], Validation Accuracy: 96.94%, Validation Loss: 0.1027\n",
            "Epoch 13, Current Learning Rate: 0.004256056901533674\n",
            "Epoch [13/20], Loss: 0.0665\n",
            "Epoch [13/20], Validation Accuracy: 97.04%, Validation Loss: 0.1006\n",
            "Epoch 14, Current Learning Rate: 0.00427739497666148\n",
            "Epoch [14/20], Loss: 0.0613\n",
            "Epoch [14/20], Validation Accuracy: 97.22%, Validation Loss: 0.0963\n",
            "Epoch 15, Current Learning Rate: 0.004298733051789286\n",
            "Epoch [15/20], Loss: 0.0571\n",
            "Epoch [15/20], Validation Accuracy: 97.24%, Validation Loss: 0.0948\n",
            "Epoch 16, Current Learning Rate: 0.004320071126917093\n",
            "Epoch [16/20], Loss: 0.0527\n",
            "Epoch [16/20], Validation Accuracy: 97.31%, Validation Loss: 0.0933\n",
            "Epoch 17, Current Learning Rate: 0.004341409202044899\n",
            "Epoch [17/20], Loss: 0.0491\n",
            "Epoch [17/20], Validation Accuracy: 97.38%, Validation Loss: 0.0914\n",
            "Epoch 18, Current Learning Rate: 0.004362747277172705\n",
            "Epoch [18/20], Loss: 0.0455\n",
            "Epoch [18/20], Validation Accuracy: 97.34%, Validation Loss: 0.0892\n",
            "Epoch 19, Current Learning Rate: 0.004384085352300511\n",
            "Epoch [19/20], Loss: 0.0424\n",
            "Epoch [19/20], Validation Accuracy: 97.35%, Validation Loss: 0.0891\n",
            "Epoch 20, Current Learning Rate: 0.004405423427428318\n",
            "Epoch [20/20], Loss: 0.0395\n",
            "Epoch [20/20], Validation Accuracy: 97.34%, Validation Loss: 0.0881\n",
            "Accuracy on test set: 97.68%\n",
            "Precision on test set: 0.9766\n",
            "Recall on test set: 0.9766\n",
            "F1 Score on test set: 0.9766\n",
            "Data has been written to tensorboard_logs_SGD-Nesterov_OneCycleLR.xlsx\n"
          ]
        }
      ],
      "source": [
        "lr_rates = []\n",
        "# Loop over each optimizer and scheduler\n",
        "for optimizer_name, optimizer_fn in optimizers:\n",
        "    for scheduler_fn in schedulers:\n",
        "        # Skip OneCycleLR with Adagrad because it doesn't work\n",
        "        if optimizer_name == \"Adagrad\" and scheduler_fn == schedulers[1]:\n",
        "            continue\n",
        "\n",
        "        # Initialize the network and optimizer\n",
        "        model = Net()\n",
        "        optimizer = optimizer_fn(model.parameters())\n",
        "\n",
        "        # Initialize the learning rate scheduler, if any\n",
        "        scheduler = scheduler_fn(optimizer) if scheduler_fn else None\n",
        "\n",
        "        # Log the optimizer and scheduler names\n",
        "        scheduler_name = type(scheduler).__name__ if scheduler else \"NoScheduler\"\n",
        "\n",
        "        # Create a directory for logs\n",
        "        logdir = os.path.join(\"runs\", f'{optimizer_name}-{scheduler_name}')\n",
        "\n",
        "        # Initialize the SummaryWriter with the log directory\n",
        "        writer = SummaryWriter(logdir)\n",
        "\n",
        "        # Define the loss function\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Training loop\n",
        "        num_epochs = 20  # Number of epochs to train\n",
        "        for epoch in range(1, num_epochs + 1):\n",
        "            total_loss = 0  # Initialize total loss for the epoch\n",
        "            model.train()\n",
        "\n",
        "            for batch_idx, (data, target) in enumerate(train_loader):\n",
        "                optimizer.zero_grad()  # Clear the gradients\n",
        "                output = model(data)   # Forward pass: compute the model output\n",
        "                loss = criterion(output, target)  # Compute the loss\n",
        "                loss.backward()  # Backward pass: compute gradient of the loss w.r.t. model parameters\n",
        "                optimizer.step()  # Update model parameters\n",
        "                total_loss += loss.item()  # Accumulate the loss\n",
        "\n",
        "            avg_loss = total_loss / train_batch_size\n",
        "            writer.add_scalar(\"Train Loss\", avg_loss, epoch)  # Log the test accuracy\n",
        "\n",
        "            current_lr = optimizer.param_groups[0]['lr']\n",
        "            lr_rates.append(current_lr)\n",
        "            print(f'Epoch {epoch}, Current Learning Rate: {current_lr}')\n",
        "\n",
        "            if scheduler:\n",
        "                if isinstance(scheduler, lr_scheduler.ReduceLROnPlateau):\n",
        "                    scheduler.step(avg_loss)\n",
        "                else:\n",
        "                    scheduler.step()\n",
        "\n",
        "            # Print training status\n",
        "            print(f\"Epoch [{epoch}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "            model.eval()  # Set the model to evaluation mode\n",
        "            val_correct = 0\n",
        "            val_total = 0\n",
        "            val_loss = 0\n",
        "            with torch.no_grad():  # Disable gradient calculation for evaluation\n",
        "                for data, target in val_loader:\n",
        "                    output = model(data)  # Forward pass: compute the model output\n",
        "                    val_loss += criterion(output, target).item()  # Compute the loss\n",
        "                    _, predicted = torch.max(output.data, 1)  # Get the index of the max log-probability (predicted class)\n",
        "                    val_total += target.size(0)  # Accumulate the total number of samples\n",
        "                    val_correct += (predicted == target).sum().item()  # Accumulate the number of correct predictions\n",
        "\n",
        "            val_accuracy = 100 * val_correct / val_total  # Calculate accuracy as a percentage\n",
        "            writer.add_scalar(\"Validation Accuracy\", val_accuracy, epoch)  # Log the validation accuracy\n",
        "            writer.add_scalar(\"Validation Loss\", val_loss / len(val_loader), epoch)  # Log the validation loss\n",
        "            print(f\"Epoch [{epoch}/{num_epochs}], Validation Accuracy: {val_accuracy:.2f}%, Validation Loss: {val_loss / len(val_loader):.4f}\")\n",
        "\n",
        "        # Evaluation on test set\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        all_targets = []\n",
        "        all_predictions = []\n",
        "        with torch.no_grad():  # Disable gradient calculation for evaluation\n",
        "            for data, target in test_loader:\n",
        "                output = model(data)  # Forward pass: compute the model output\n",
        "                _, predicted = torch.max(output.data, 1)  # Get the index of the max log-probability (predicted class)\n",
        "                total += target.size(0)  # Accumulate the total number of samples\n",
        "                correct += (predicted == target).sum().item()  # Accumulate the number of correct predictions\n",
        "                all_targets.extend(target.cpu().numpy())\n",
        "                all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "        # Calculate different metrics for the test set\n",
        "        accuracy = 100 * correct / total\n",
        "        precision = precision_score(all_targets, all_predictions, average='macro')\n",
        "        recall = recall_score(all_targets, all_predictions, average='macro')\n",
        "        f1 = f1_score(all_targets, all_predictions, average='macro')\n",
        "\n",
        "        # Write the metrics to the event file\n",
        "        writer.add_scalar(\"Test Accuracy\", accuracy)  # Log the test accuracy\n",
        "        writer.add_scalar(\"Test Precision\", precision)\n",
        "        writer.add_scalar(\"Test Recall\", recall)\n",
        "        writer.add_scalar(\"Test F1 Score\", f1)\n",
        "\n",
        "        # Console output\n",
        "        print(f'Accuracy on test set: {accuracy:.2f}%')\n",
        "        print(f'Precision on test set: {precision:.4f}')\n",
        "        print(f'Recall on test set: {recall:.4f}')\n",
        "        print(f'F1 Score on test set: {f1:.4f}')\n",
        "\n",
        "        writer.close()  # Close the TensorBoard writer\n",
        "\n",
        "        # =======================\n",
        "        # Save TensorBoard Logs\n",
        "        # =======================\n",
        "\n",
        "        # Specify the directory where the TensorBoard logs are saved\n",
        "        event_file = f'{optimizer_name}-{scheduler_name}'\n",
        "        log_dir = f'./runs/{event_file}'\n",
        "\n",
        "        # Get the list of event files in the directory\n",
        "        event_files = [os.path.join(log_dir, f) for f in os.listdir(log_dir) if 'events.out.tfevents' in f]\n",
        "\n",
        "        # Initialize an empty list to store data\n",
        "        data = []\n",
        "\n",
        "        # Initialize an event accumulator for each event file\n",
        "        for event_file in event_files:\n",
        "            ea = event_accumulator.EventAccumulator(event_file)\n",
        "            ea.Reload()  # Load the event file\n",
        "\n",
        "            # Extract scalar data\n",
        "            tags = ea.Tags()['scalars']\n",
        "            for tag in tags:\n",
        "                events = ea.Scalars(tag)\n",
        "                for event in events:\n",
        "                    step = event.step\n",
        "                    value = event.value\n",
        "                    wall_time = event.wall_time\n",
        "                    data.append([tag, step, value, wall_time])\n",
        "\n",
        "        # Convert the data into a DataFrame\n",
        "        df = pd.DataFrame(data, columns=['Tag', 'Step', 'Value', 'Wall time'])\n",
        "\n",
        "        # Save the DataFrame to an Excel file\n",
        "        output_file = f'tensorboard_logs_{optimizer_name}_{scheduler_name}.xlsx'\n",
        "        df.to_excel(output_file, index=False)\n",
        "\n",
        "        print(f\"Data has been written to {output_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtCaZQbCU85G"
      },
      "source": [
        "Logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXF2hWgb9CSK",
        "outputId": "0206f757-8958-4edd-86e3-d505a894402d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data has been written to tensorboard_logs.xlsx\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from tensorboard.backend.event_processing import event_accumulator\n",
        "\n",
        "# Specify the directory where the TensorBoard logs are saved\n",
        "'''\n",
        "!!! Hier anpassen, welches Event ihr auslesen wollt. !!!\n",
        "'''\n",
        "event_file = '07-16_12-01-Adam-PolynomialLR'\n",
        "\n",
        "log_dir = f'./runs/{event_file}'\n",
        "\n",
        "# Get the list of event files in the directory\n",
        "event_files = [os.path.join(log_dir, f) for f in os.listdir(log_dir) if 'events.out.tfevents' in f]\n",
        "\n",
        "# Initialize an empty list to store data\n",
        "data = []\n",
        "\n",
        "# Initialize an event accumulator for each event file\n",
        "for event_file in event_files:\n",
        "    ea = event_accumulator.EventAccumulator(event_file)\n",
        "    ea.Reload()  # Load the event file\n",
        "\n",
        "    # Extract scalar data\n",
        "    tags = ea.Tags()['scalars']\n",
        "    for tag in tags:\n",
        "        events = ea.Scalars(tag)\n",
        "        for event in events:\n",
        "            step = event.step\n",
        "            value = event.value\n",
        "            wall_time = event.wall_time\n",
        "            data.append([tag, step, value, wall_time])\n",
        "\n",
        "# Convert the data into a DataFrame\n",
        "df = pd.DataFrame(data, columns=['Tag', 'Step', 'Value', 'Wall time'])\n",
        "\n",
        "# Save the DataFrame to an Excel file\n",
        "output_file = 'tensorboard_logs.xlsx'\n",
        "df.to_excel(output_file, index=False)\n",
        "\n",
        "print(f\"Data has been written to {output_file}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
